# Kalypso Observability Hub

Kalypso Observability Hub is a central storage that contains deployment data with historical information on workload versions and their deployment state across hosts. This data is used by dashboards and alerts for the deployment monitoring purposes, by the CD pipelines, implementing progressive rollout across environments and by various external systems that make decisions basing on the deployment data. 

## Motivation

Platform and Application Dev teams need a deployment observability solution to perform the following activities:

- monitor what application/service versions are deployed to clusters/hosts in the environments
- compare environments and see deployment discrepancy (e.g. how my "stage" environment is different from "prod")
- track deployment history per environment, per application/service, per microservice 
- compare desired deployment state to the reality and see deployment drift

## Deployment reports

The challenges, mentioned above, can be addressed, to some extent, if Platform and Application Dev teams have dashboards and alerts that provide information as in [these mockup examples](./docs/images/DeploymentObservabilityReports.png).

## Architecture

The high level architecture of the solution is represented on the following diagram:

![deployment-observability-high-level](./docs/images/deployment-observability-high-level.png)

### Components

At high level Deployment Observability Hub consists of `storage`, `API` and `controller`.

#### Storage

Storage is a database containing the deployment observability data. The data can serve as a datasource for the tools, such as Grafana. 
There is not any requirements regarding the exact database implementation. In this implementation, PostgreSQL has been selected. 

#### API

External systems, for example GitHub CD workflows, may query the deployment observability hub regarding the state of the rollout. They communicate with the hub via API, exposed with OpenAPI.

#### Controller

Controller is a standard [K8s controller](https://kubernetes.io/docs/concepts/architecture/controller/) that collects data from the deployment descriptors and hosts and saves the data to the hub storage. See the [detailed architecture diagram](./docs/images/deployment-observability-hub.png) unfolding the controller components.

#### Storage data model

See [logical data model](./docs/images/DeploymentObservabilityLogicalModel.png) of the Observability Hub storage.

### Data flow

Deployment Observability Hub is designed to store large amount of deployment data, that is easy to query with complex queries and consume in various dashboards and alerts. 

The data is coming to the observability hub from two different sources.

#### Desired deployment state

High-level information, describing the desired state of the deployments, which includes environments, application versions, deployment targets, etc. is transferred to the observability hub from the application GitOps repositories. Application CI/CD pipelines submit to the GitOps repositories K8s manifests along with the deployment descriptors, describing what these manifests are all about. See [an example](#example) of a deployment descriptor, generated by the CD flow and submitted to an application GitOps repository.

By collecting the deployment descriptors data in the deployment observability hub, we'll be able to provide information about what is supposed to be deployed where across environments. 
The deployment descriptors are delivered to the observability hub K8s cluster from the GitOps repositories by Flux. These resources are watched and processed by the observability hub controller. The controller saves the normalized data into the hub storage.  

#### Real deployment state

The facts of actual deployment are coming from the hosts. For example, Azure Arc GitOps extension, installed on the hosts, reports to Azure Resource Graph the compliance status with the GitOps repositories. It reports what GitOps commit has been deployed to a host, when and in what status. This deployment state data is pulled from ARG to the observability hub and watched by the observability hub controller. 

Depending on the host platform and the reconciler implementation, the delivery channel of the deployment state data to the observability hub may vary. It can be implemented on top of Azure Arc GitOps extensions and ARG, as described above for the Azure Arc enabled K8s clusters; it can be built on top of OpenTelemetry protocol with a setup of Otel Collectors on the host and the observability hub; it can use custom agents, running on the hosts and submitting deployment state directly to the observability hub; or it can be a combination of those approaches. 

At this point the observability hub implementation is focused on K8s clusters with Azure Arc GitOps extension installed. The observability hub controller provides out-of-the-box functionality to pull deployment state from the Azure Resource Graph.

The "desired" and the "real" deployment states correlate with each other by the means of commit Id and compose the whole picture of the deployment state.

## Observability hub abstractions

### Deployment Descriptor
  
#### Example

```yaml
apiVersion: hub.kalypso.io/v1alpha1
kind: DeploymentDescriptor
metadata:
  name: hello-world-functional-test-0-0-1
spec:
  workload:
    name: hello-world
    source:
        repo: https://github.com/kaizentm/hello-world
        branch: main
        path: .
    application:
        name: greeting-service
        workspace:
            name: kaizentm
  deploymentTarget:
    name: functional-test
    environment: dev
    manifests:
        repo: https://github.com/kaizentm/hello-world-gitops
        branch: dev
        path: functional-test
  workloadVersion:
    version: 0.0.1
    build: build-1
    commit: ca9ee9d0ff9ec52b998fdcf64e128c84ddd0e661
    buildTime: "2023-04-27T23:25:05Z"
```

### Cluster

#### Example

### Deployment

#### Example

## Observability hub API

## Installation

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
